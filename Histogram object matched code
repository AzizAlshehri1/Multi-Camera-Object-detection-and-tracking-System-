!pip istall ultralytics

/////////////////////////////




import cv2
from ultralytics import YOLO
import numpy as np
from collections import defaultdict

# Load pretrained YOLO model
model = YOLO("yolov10x.pt")

# Define paths to video files
source_video1 = "/content/VideoTracking11.mp4"
source_video2 = "/content/VideoTracking1.mp4"

# Open the video files
cap1 = cv2.VideoCapture(source_video1)
cap2 = cv2.VideoCapture(source_video2)

# Get width, height, and FPS for both videos
width1 = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))
height1 = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps1 = int(cap1.get(cv2.CAP_PROP_FPS))

width2 = int(cap2.get(cv2.CAP_PROP_FRAME_WIDTH))
height2 = int(cap2.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps2 = int(cap2.get(cv2.CAP_PROP_FPS))

# Define codecs and create VideoWriter objects to save the output videos
output_path1 = "/content/output_video1_with_tracking.mp4"
output_path2 = "/content/output_video2_with_tracking.mp4"
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out1 = cv2.VideoWriter(output_path1, fourcc, fps1, (width1, height1))
out2 = cv2.VideoWriter(output_path2, fourcc, fps2, (width2, height2))

# Function to calculate IoU
def intersection_over_union(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
    iou = interArea / float(boxAArea + boxBArea - interArea) if (boxAArea + boxBArea - interArea) > 0 else 0
    return iou

# Function to compute color histogram
def compute_histogram(image):
    # Convert image to HSV color space
    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    histogram = cv2.calcHist([hsv_image], [0, 1], None, [8, 8], [0, 180, 0, 256])
    cv2.normalize(histogram, histogram)
    return histogram

# Store tracked IDs and positions for both videos
tracked_ids1 = {}
tracked_ids2 = {}
color_histograms1 = {}
color_histograms2 = {}

next_id1 = 1  # Next available ID for video 1
next_id2 = 1  # Next available ID for video 2

# Process frames from the first video
while True:
    ret1, frame1 = cap1.read()
    if not ret1:
        break  # Break the loop if there are no more frames

    results1 = model.track(frame1, stream=False)

    current_boxes1 = []

    for result in results1:
        for box in result.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            current_boxes1.append((x1, y1, x2, y2))

            # Compute histogram for the detected car
            car_image = frame1[y1:y2, x1:x2]
            car_hist = compute_histogram(car_image)

            # Find existing ID or assign a new one
            assigned_id = None
            for tid, (bx1, by1, bx2, by2) in tracked_ids1.items():
                if intersection_over_union((bx1, by1, bx2, by2), (x1, y1, x2, y2)) > 0.5:  # IoU threshold
                    assigned_id = tid
                    break

            # If no existing ID, check against color histograms from video 2
            if assigned_id is None:
                for tid2, hist2 in color_histograms2.items():
                    similarity = cv2.compareHist(car_hist, hist2, cv2.HISTCMP_CORREL)
                    if similarity > 0.7:  # Similarity threshold
                        assigned_id = tid2
                        break

            # If still no ID, assign a new one
            if assigned_id is None:
                assigned_id = next_id1
                next_id1 += 1

            tracked_ids1[assigned_id] = (x1, y1, x2, y2)
            color_histograms1[assigned_id] = car_hist

            # Draw bounding box and label for the first video
            cv2.rectangle(frame1, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame1, f"ID {assigned_id}", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    out1.write(frame1)

# Process frames from the second video
while True:
    ret2, frame2 = cap2.read()
    if not ret2:
        break  # Break the loop if there are no more frames

    results2 = model.track(frame2, stream=False)

    current_boxes2 = []

    for result in results2:
        for box in result.boxes:
            x3, y3, x4, y4 = map(int, box.xyxy[0])
            current_boxes2.append((x3, y3, x4, y4))

            # Compute histogram for the detected car
            car_image = frame2[y3:y4, x3:x4]
            car_hist = compute_histogram(car_image)

            # Find existing ID or assign a new one
            assigned_id = None
            for tid, (bx1, by1, bx2, by2) in tracked_ids2.items():
                if intersection_over_union((bx1, by1, bx2, by2), (x3, y3, x4, y4)) > 0.5:  # IoU threshold
                    assigned_id = tid
                    break

            # If no existing ID, check against color histograms from video 1
            if assigned_id is None:
                for tid1, hist1 in color_histograms1.items():
                    similarity = cv2.compareHist(car_hist, hist1, cv2.HISTCMP_CORREL)
                    if similarity > 0.7:  # Similarity threshold
                        assigned_id = tid1
                        break

            # If still no ID, assign a new one
            if assigned_id is None:
                assigned_id = next_id2
                next_id2 += 1

            tracked_ids2[assigned_id] = (x3, y3, x4, y4)
            color_histograms2[assigned_id] = car_hist

            # Draw bounding box and label for the second video
            cv2.rectangle(frame2, (x3, y3), (x4, y4), (0, 255, 0), 2)
            cv2.putText(frame2, f"ID {assigned_id}", (x3, y3 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    out2.write(frame2)

# Release resources
cap1.release()
cap2.release()
out1.release()
out2.release()

print(f"Output video 1 with tracking saved at {output_path1}")
print(f"Output video 2 with tracking saved at {output_path2}")